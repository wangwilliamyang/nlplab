\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[pdftex]{graphicx}
\usepackage{amsfonts,eucal,amsbsy,amsopn,amsmath}
\usepackage{url}
\usepackage[sort&compress]{natbib}
\usepackage{natbibspacing}
\usepackage{latexsym}
\usepackage{wasysym} 
\usepackage{rotating}
\usepackage{fancyhdr}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{sectsty}
\usepackage[dvipsnames,usenames]{color}
\usepackage{multicol}
\definecolor{orange}{rgb}{1,0.5,0}
\usepackage{multirow}
\usepackage{sidecap}
\usepackage{caption}
\renewcommand{\captionfont}{\small}
\setlength{\oddsidemargin}{-0.04cm}
\setlength{\textwidth}{16.59cm}
\setlength{\topmargin}{-0.04cm}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{22.94cm}
\allsectionsfont{\normalsize}
\newcommand{\ignore}[1]{}
\newcommand{\thedate}{\today}
\newenvironment{enumeratesquish}{\begin{list}{\addtocounter{enumi}{1}\arabic{enumi}.}{\setlength{\itemsep}{-0.25em}\setlength{\leftmargin}{1em}\addtolength{\leftmargin}{\labelsep}}}{\end{list}}
\newenvironment{itemizesquish}{\begin{list}{\setcounter{enumi}{0}\labelitemi}{\setlength{\itemsep}{-0.25em}\setlength{\labelwidth}{0.5em}\setlength{\leftmargin}{\labelwidth}\addtolength{\leftmargin}{\labelsep}}}{\end{list}}

\bibpunct{(}{)}{;}{a}{,}{,}
\newcommand{\nascomment}[1]{\textcolor{blue}{\textbf{[#1 --NAS]}}}


\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{\thepage~of \pageref{lastpage}}
\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}


\title{11-712:  NLP Lab Report}
\author{William Yang Wang\\
ww@cmu.edu}
%\data{January 17, 2014}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
Dependency parsing is a core task in NLP, and it is a widely used by many applications such as information extraction,
question answering, and machine translation. In general, the resources for Chinese dependency parsing are less accessible than English, and publicly available Chinese dependency parsers are still very limited. In this project, the goal is to build a Chinese dependency parser that can be used by others.
\end{abstract}

\section{Basic Information about Chinese Dependency Parsing}
\label{sec:info}
Chinese dependency parsing has attracted many interests in the past decade.
\cite{bikel2000two} and \citet{Chiang:2002} are among the first to use Penn Chinese Tree Bank for dependency parsing,
where they adapted~\cite{xia1999extracting}'s head rules.
A few years later, the CoNLL shared task opened a track for multilingual dependency parsing,
which also included Chinese~\citep{buchholz2006conll,nilsson2007conll}.
These shared tasks soon popularized Chinese dependency parsing by making datasets available,
and there has been growing amount of literature since then~\citep{zhang2008tale,nivre2007maltparser,sagae2007dependency,
che2010ltp,carreras2007experiments,duan2007probabilistic}.
In this work, we aim at building a new publicly available Chinese dependency parsing tool, using 
new technologies that aim at improving the accuracy of the state-of-the-art.

\section{Past Work on the Syntax of Chinese}
\cite{chao1968grammar} is among the first to study the syntax of Chinese. Unlike 
English, there has been long debate on the wordhood of Chinese~\citep{duanmu1998wordhood}.
Chao and others' work investigate the free and bound forms,
prosodic aspects~\citep{shen1990prosody}, semantics~\cite{li1972semantics,wu1999syntax}, and morphological aspects~\citep{tang1989studies,dai1992chinese,sproat2002corpus} to define the unit of word in Chinese.
In addition, he has also studied the complex compound constructions~\citep{zhou1999morphology,zhang2000extraction} in Chinese,
as well as the parts of speech such as nouns and verbs~\citep{krifka1995common}.
More recently, \cite{huang2009syntax} have studied
the lexical and functional categories,
argument structure, and the verb phrase in Chinese.
Moreover, they have discussed the more unique and challenging parts
of syntax in Chinese: the passives, the \emph{ba} construction,
and the topic \& relative constructions.
Interestingly, they have also shed light on some advanced Chinese linguistic issues 
that have not been well studied in the past: 
questions, nominal expressions, and anaphora.

Even though there has been many interesting linguistics papers on various aspects
of syntax in Chinese, the corresponding computational modeling work has been rather limited.
One of the most popular computational tasks in Chinese NLP is word segmentation~\citep{xue2003chinese,sproat2003first}.
where researchers have previously investigated sequencial models such as
hierarchial hidden Markov model~\citep{zhang2003hhmm}, maximum entropy Markov model~\citep{xue2003chinese}, and conditional random fields~\citep{zhao2006improved} for this task.
In addition to tokenization and segmentation, standard structure prediction tasks such as 
named entity recognition~\citep{xue2003chinese,wu2005chinese}, part-of-speech tagging~\citep{ng2004chinese,jiang2008cascaded}, and constituency parsing~\citep{wu1997stochastic,wang2006fast}
have also been studied in the language-specific setups.
As mentioned in Section~\ref{sec:info}, Chinese dependency parsing was first introduced
by \cite{bikel2000two}, and then became popular after the CoNLL multilingual shared tasks~\citep{buchholz2006conll,nilsson2007conll}.

In the past decade, there have been growing number of publicly availble Chinese language processing tools.
ICTCLAS\footnote{http://sewm.pku.edu.cn/QA/reference/ICTCLAS/FreeICTCLAS/English.html} is one of the most popular word segmentation tool in Chinese NLP. The Stanford Chinese NLP constituency parser~\citep{levy2003harder},
and the dependency parser~\citep{chang2009discriminative} also provide insights for many Chinese NLP applications.
More recently, more comphrehensive and Chinese-optimized toolkits were also made available~\citep{qiu2013fudannlp,che2010ltp}.
To the best of my knowledge, even though systems such as Malt parser~\citep{nivre2007maltparser} provides solutions 
to multilingual dependency parsing, but they are not optimized for Chinese, and the accuracy on Penn Chinese Treebank is
typically around 70\% and lower 80\%, which falls behind languages like English and German.

\section{Available Resources}
After some research and hands-on experiments on real data, I decided to use the open-source Stanford Word Segmenter\footnote{http://nlp.stanford.edu/software/segmenter.shtml} as the
segmentation tool. Comparing to other popular Chinese word segmenters, the Stanford segmenter
is well-maintained, and well-documented. The open-source Chinese lexicon I plan to use
is also attached in the distribution of Stanford Chinese Segmenter: the Penn Tree Bank lexicon
and the PKU lexicon. For the Chinese reference grammar, I am currently investigating the 
Stanford Dependencies\footnote{http://nlp.stanford.edu/software/stanford-dependencies.shtml},
but I am also open to other suggestions.

The test data that I am considering using at this stage: A. a subset of Wang Ling's $\mu$topia Chinese microblog dataset\footnote{http://www.cs.cmu.edu/~lingwang/microtopia/}; B. a subset of the test data from HIT-SCIR's LTP test program\footnote{https://github.com/HIT-SCIR/ltp/blob/master/test\_data/test\_utf8.txt}.
I chose the above two datasets because they belong to very different genres:
Wang Ling's data is from Sina Weibo, while the second one is taken from newswire headlines.
It might be interesting to compare the difficulty in the dependency annotation process,
as well as parsing results.

\section{Survey of Phenomena in Chinese Dependency Parsing}

\section{Initial Design}

\section{System Analysis on Corpus A}

\section{Lessons Learned and Revised Design}

\section{System Analysis on Corpus B}

\section{Final Revisions}

\section{Future Work}





\bibliographystyle{plainnat}
\bibliography{parsing}

\label{lastpage}
\end{document}
